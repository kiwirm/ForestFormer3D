
ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest
LiDAR 3D Point Clouds
Binbin Xiang1
, Maciej Wielgosz1
, Stefano Puliti1
, Kamil Kral¬¥
2
Martin KruÀöcek Àá
2
, Azim Missarov2
, Rasmus Astrup1
1Norwegian Institute of Bioeconomy Research (NIBIO)
2Silva Tarouca Research Institute for Landscape and Ornamental Gardening
{binbin.xiang, maciej.wielgosz, stefano.puliti, rasmus.astrup}@nibio.no
{kral, krucek, missarov}@vukoz.cz
Abstract
The segmentation of forest LiDAR 3D point clouds, including both individual tree and semantic segmentation, is fundamental for advancing forest management and ecological research. However, current approaches often struggle with the complexity and variability of natural forest
environments. We present ForestFormer3D, a new unified
and end-to-end framework designed for precise individual
tree and semantic segmentation. ForestFormer3D incorporates ISA-guided query point selection, a score-based block
merging strategy during inference, and a one-to-many association mechanism for effective training. By combining
these new components, our model achieves state-of-the-art
performance for individual tree segmentation on the newly
introduced FOR-instanceV2 dataset, which spans diverse
forest types and regions. Additionally, ForestFormer3D
generalizes well to unseen test sets (Wytham woods and
LAUTx), showcasing its robustness across different forest
conditions and sensor modalities. The FOR-instanceV2
dataset and the ForestFormer3D code are publicly available at https://bxiang233.github.io/FF3D/.
1. Introduction
Forests, covering approximately 31% of the Earth‚Äôs land
surface, are essential for biodiversity conservation, carbon
storage, provision of timber and a wide range of ecosystem services central to the quality of life. Forests are complex 3D structures and to fully understand and map the
forest ecosystems requires a detailed 3D representation of
the structure. Advancements in high-density LiDAR technology now enable large-scale capture of 3D point clouds
with unprecedented detail, paving the way for improved
and streamlined forest ecosystem data capture. However, to
capitalize on the captured data, further advancement in semantic and instance segmentation of 3D forest point clouds
is still required. Significant progress has been made in
3D segmentation for indoor and urban environments, but
moving to forest scenes introduces a substantial increase
in complexity. The organic, intertwined structure of trees,
highly variable sizes and shapes, and intricate occlusion patterns demand specialized segmentation methods. In simpler
forest structures, like boreal forests, LiDAR-derived point
clouds have been shown to enable tasks such as forest semantic segmentation, tree instance segmentation, and tree
biophysical parameter estimation [63]. However, in more
complex broadleaved, temperate, or tropical forests, increased structural complexity and species richness demand
specialized and novel segmentation methods beyond current
approaches [58], which can address the following key unresolved challenges:
‚Ä¢ Structural complexity. Dense canopies, characterized
by closely spaced trees with plastic and overlapping tree
crowns, complicate the separation of individual trees (see
Fig. 1(a)).
‚Ä¢ Multi-scale representation. Forests contain both large
canopy trees and small understory trees, requiring models capable of segmenting across multiple scales (see
Fig. 1(b)).
‚Ä¢ Variability in point distribution. Differences in acquisition sensors, forest types, and environmental conditions lead to variations in point density and distribution,
necessitating robust generalization across diverse forest
scenes and sensor configurations (see differences among
Fig. 1(a)-(c)).
Focusing on improving the state-of-the-art for more
complex forests, this paper provides two primary contributions:
‚Ä¢ FOR-instanceV2 dataset: We introduce an expanded
dataset based on FOR-instance [41], with a primary foarXiv:2506.16991v2 [cs.CV] 1 Aug 2025
(a) TUWIEN (b) BlueCat (c) Yuchen
Input Semantic Individual trees
Ground
Wood
Leaf
Input Semantic Individual trees
Ground
Wood
Leaf
Input Semantic Individual trees
Ground
Wood
Leaf
Figure 1. Examples of semantic and individual tree segmentation in different forest regions from our dataset, visualized with consistent
point size. The colors for the individual trees are assigned randomly. The figure highlights the density and distribution variability between
regions (a)‚Äì(c). (a) TUWIEN: Deciduous-dominated alluvial forest in Austria, showing dense, interwoven tree structures. (b) BlueCat:
This area includes both large and small trees, highlighting the challenge of segmenting trees of significantly varied sizes within the same
region. (c) Yuchen: A tropical forest region with tall trees and sparse points in the lower stem regions due to occlusion, adding difficulty to
segmentation tasks, particularly for stem part.
cus on increasing forest structural complexity by incorporating broadleaved temperate and tropical forests. This
dataset encompasses a wider range of forest types and regions, establishing a robust benchmark for forest point
cloud segmentation tasks. This dataset includes dedicated
training, validation, and test sets, enabling comprehensive
evaluation of segmentation accuracy and robustness.
‚Ä¢ ForestFormer3D: We propose ForestFormer3D, a unified, end-to-end framework for individual tree segmentation and semantic segmentation of forest point clouds.
ForestFormer3D achieves state-of-the-art performance
for individual tree segmentation on the FOR-instanceV2
test set and demonstrates strong generalization capabilities on two previously unseen test regions (Wytham
woods and LAUTx), highlighting its adaptability and effectiveness across diverse forest types.
2. Related work
3D point cloud segmentation. 3D point cloud semantic and instance segmentation are fundamental tasks in 3D
computer vision. Semantic segmentation assigns each point
in a point cloud to a predefined category, typically supervised by cross-entropy loss and developed through advancements in network architectures and computational strategies [10, 13, 42, 48]. Instance segmentation extends this by
assigning a unique instance label to each point, allowing for
the distinction of individual objects within each category.
Much research in 3D point cloud segmentation has focused on indoor scenes, with benchmarks such as ScanNet [15] and S3DIS [2] commonly used for evaluation. Outdoor segmentation has primarily targeted applications in autonomous driving, utilizing datasets like SemanticKITTI [6]
and nuScenes [7], which focus on traffic-related classes
such as pedestrians and vehicles. However, these existing
methods and datasets are optimized for structured urban environments, leaving more complex, unstructured natural 3D
scenes, such as forests, comparatively underexplored.
Existing deep learning approaches for instance segmentation in 3D point clouds can be divided into three main
types: top-down, bottom-up and transformer-based methods. Top-down approaches typically detect objects first, often using bounding boxes or proposals, and then apply binary mask prediction within these detected regions [29, 64,
66, 67]. These methods rely on the initial detection stage
to localize objects accurately. Bottom-up approaches focus on extracting discriminative point-level features, which
are then clustered into instances [11, 19, 20, 22, 26, 33,
39, 51, 55, 69]. These methods are capable of capturing
fine-grained details but are prone to issues such as oversegmentation or under-segmentation. Moreover, their reliance on clustering hyperparameters introduces additional
challenges in tuning these methods for complex, variabledensity point clouds.
Transformer-based approaches have recently emerged as
a promising solution, offering fully end-to-end frameworks
that leverage attention mechanisms to model complex spatial relationships while avoiding error accumulation through
intermediate stages [30, 43, 47]. This unified design allows
for joint training of both semantic and instance segmentation tasks, potentially enhancing performance through mutual learning effects [30]. In these architectures, a 3D backbone such as sparse 3D U-Net [13] extracts global features
from the point cloud, which are processed by a transformer
decoder with a fixed set of queries to concurrently predict semantic and instance masks [30, 43, 47]. Recent research has further optimized this framework, focusing on
query initialization and refinement techniques to improve
alignment between instance queries and object locations,
thereby reducing the need for extensive iterative refinement [37, 47]. Additionally, integrating geometric and se-
mantic information has proven effective in enhancing mask
quality and boundary precision by addressing limitations in
spatial cues and fine-grained detail capture during query refinement [44, 65]. To handle multi-scale representations
and maintain spatial consistency in large scenes, techniques
capturing both local details and global context have been
introduced, further advancing segmentation performance in
diverse and complex environments [35, 36].
Forest 3D scene segmentation. Forest point cloud segmentation has long been a significant challenge [25, 40]
aimed primarily at localizing and segmenting individual
tree crowns [4, 16, 21, 34, 59], and tree semantic segmentation, distinguishing for example leaf from wood [50, 52,
53]. These methods often rely on density-based techniques,
hand-crafted features, or canopy height models (CHMs),
which are effective in certain controlled settings but lack
adaptability across varied forest types, requiring extensive
manual adjustments and showing poor generalizability.
Accurate instance and semantic segmentation of forest point clouds could fundamentally streamline numerous forestry tasks, transforming individual tree detection,
species classification, and biophysical parameter prediction into by-products of a comprehensive segmentation
model. The growing availability of high-density LiDAR
data, along with open labelled datasets [1, 5, 8, 41, 58],
provide the foundation necessary for deep learning methods to achieve automated and adaptable forest point cloud
segmentation [9, 12, 27, 28, 31, 45, 46, 54, 57, 61, 68, 70].
However, progress in forest segmentation lags behind developments in urban and indoor contexts. The majority
of forestry research has focused on semantic segmentation
tasks, such as separating leaf and wood points [27, 28, 31,
61]. Few studies focus on individual tree segmentation,
with most relying on multi-step, hand-crafted pipelines that
limit their applicability across diverse forest types and sensor setups [57, 60].
To bridge this gap, recent frameworks aim to integrate
multiple segmentation tasks into a single model [24, 58,
63]. For example, the ForAINet framework combines semantic and instance segmentation using a bottom-up approach [63]. While effective in most simple forest environments, this model tends to over-segment trees in densely
packed areas and fails to accurately identify small trees in
multi-layered forests.
In this paper, we present ForestFormer3D, the first endto-end framework for individual tree segmentation and semantic segmentation tailored specifically for forest point
clouds. Unlike previous bottom-up methods that rely
on non-differentiable clustering steps for individual tree
segmentation [24, 58, 63], our transformer-based decoder
enables fully differentiable, end-to-end training. Moreover, ForestFormer3D eliminates the need for intermediate
matching by directly supervising predicted masks with their
corresponding ground truth instances. This approach offers both a streamlined architecture and improved individual tree segmentation accuracy. The key innovations (see
Fig. 2) in our framework include:
‚Ä¢ We introduce both an instance- and a semantic-aware
guided (ISA-guided) query point selection strategy to
generate high-quality query points, helping in balancing
precision and recall in individual tree segmentation and
ensuring more complete instance coverage.
‚Ä¢ Leveraging the known positions of query points, we apply a one-to-many association method for efficient mask
supervision, replacing the Hungarian-based one-to-one
matching strategy commonly adopted in earlier methods.
‚Ä¢ To process large-scale forest point clouds, we implement
a score-based global ranking block-merging strategy, allowing seamless integration across extensive scenes.
3. Method
Our method aims at the combined task of semantic and individual tree segmentation in forest point clouds. The general
framework of ForestFormer3D is shown in Fig. 2. The semantic segmentation involves three primary classes in forest environments: ground, wood and leaf. Unlike indoor
datasets, which commonly include RGB and mesh information [3, 15], forest point clouds are usually captured by
LiDAR. Although LiDAR data often provides additional
information such as intensity, number of returns, and return numbers, these attributes are not consistently available across all datasets. Therefore, to ensure our method
is broadly applicable, we use only the 3D coordinates.
At first, we voxelize the input point cloud and create a
sparse tensor. Then, we process this tensor through a sparse
3D U-Net [13] to extract voxel-wise features. These features are used for both instance and semantic segmentation. We use a novel ISA-guided sampling strategy to select
high-quality query points for individual tree segmentation.
The selected instance query points, along with the semantic
query points, are fed into the query decoder. For each query
point, the decoder outputs a corresponding mask: instance
query points produce individual tree masks, while semantic
query points generate semantic region masks (see Sec. 3.1).
Since the ISA-guided sampling strategy retains the spatial information of the query points, we can directly associate each predicted mask with its corresponding tree instance, eliminating the need for one-to-one matching algorithms like the Hungarian algorithm [30, 32, 37, 43]. This
direct association allows us to efficiently compute the mask
loss, enabling the network to perform both semantic segmentation and individual tree segmentation in an end-to-end
manner (see Sec. 3.2).
Given the large-area coverage of point clouds in forest
scenes, we adopt a cropping strategy to handle large inputs. Specifically, we crop the input point cloud into cylin-
Input point cloud
ùêèùêè ‚àà ‚Ñùùëµùëµ√óùüëùüë
Sparse tensor*
ùêïùêï ‚àà ‚Ñùùë¥ùë¥√óùüëùüë
Sparse feature tensor
ùêÖùêÖ ‚àà ‚Ñùùë¥ùë¥√óùüëùüëùüëùüë
* Represents non-empty voxels as a sparse feature tensor, where ùëÄùëÄ is the number of voxels containing actual points.
MLP
Instance
discriminative
features
ùêÖùêÖùüèùüè ‚àà ‚Ñùùë¥ùë¥√óùüìùüì
Tree and
non-tree
classification
ùêÖùêÖ ‚àà ‚Ñùùë¥ùë¥√ó
Query points
Transformer decoder layer X 6
Semantic
queries
Tree instance
queries
One-to-many association
Tree instance masks
& scores
Semantic
masks
Voxelization
3D sparse
U-Net
MLP
1
2
K
1 2
K
Training:
randomly picked cylinders
Testing:
evenly spaced cylinders
Score-based block merging
ISA-guided query
point selection
Figure 2. Illustration of our ForestFormer3D framework, with key innovative components highlighted in yellow.
drical regions with a certain radius, processing one local
block at a time, as shown on the right side of Fig. 2. We
use cylindrical blocks to avoid cutting trees vertically and
to improve neighbor search efficiency [63]. During training, we use random cropping. During testing, we predict
inside each cylinder block individually, then apply a sliding window approach with a fixed stride to cover the entire
point cloud. Finally, we propose a new score-based block
merging method to handle overlapping regions, merging all
blocks into a complete point cloud prediction (see Sec. 3.3).
3.1. Network architecture
3D sparse U-Net backbone. Given an input point cloud
 \mathbf {P} \in \mathbb {R}^{N \times 3} , with N representing the number of 3D points,
we voxelize \mathbf {P} to obtain a sparse tensor \mathbf {V} \in \mathbb {R}^{M \times 3} , where
 M is the count of occupied voxels. Using the SpConvUNet [14], we extract features from \mathbf {V} , outputting a feature tensor \mathbf {F} \in \mathbb {R}^{M \times 32} .
ISA-guided query point selection. Previous research has
explored various methods for query point selection in instance segmentation tasks, as the quality of the query points
critically impacts both model performance and convergence
speed. Some approaches use learnable tensors as queries,
which are typically initialized randomly or with zero values [30]. While these parametric queries offer flexibility,
they converge slowly [38] and can be challenging to control
or interpret visually, as their initialization and refinement
are purely data-driven. Other methods, such as farthest
point sampling (FPS), select non-parametric queries directly from the raw input [43]. Non-parametric approaches
are efficient but may produce suboptimal query points that
overlook smaller instances or sample points in uninformative background regions.
To address these limitations, recent strategies have proposed more controllable query point selection methods.
For example, semantic-aware approaches prioritize selecting query points from foreground regions to avoid generating false positive masks from background classes [23].
Other methods incorporate learning-based query selection
to dynamically refine query points. These approaches typically generate initial query points and then refine their relationships or employ clustering techniques to improve the
query distribution [35, 37].
We develop a novel ISA-guided query point selection
strategy to improve the quality of the selected query points,
minimizing the likelihood of sampling background points
and ensuring more uniform and high-coverage sampling of
tree instances. Using the output tensor \mathbf {F} \in \mathbb {R}^{M \times 32} from
the 3D sparse U-Net, we process it through two parallel
Multi-Layer Perceptron (MLP) heads. The first head learns
a 5-dimensional (5D) embedding for each voxel, such that
voxels belonging to the same tree are closer in the embedding space, while voxels from different trees are pushed farther apart. This separation is achieved using an instance
discriminative loss to encourage compactness within trees
and separation between trees [17, 56, 62]. The second MLP
head performs binary classification to separate tree voxels
from non-tree voxels. During instance query point sampling, we exclude non-tree voxels and apply FPS in the 5D
embedding space of tree voxels to select a fixed number of
instance query points.
We conducted a small study to evaluate the effectiveness
of our sampling method on the FOR-instanceV2 validation
set (see Sec. 4.1). Using a sliding window approach, we
sampled 300 query points within each cylindrical region of
radius R=16\,\text {m} and a stride of R/4 . For each cylinder,
we calculated the coverage rate as the proportion of tree instances represented by selected query points and the proportion of selected query points corresponding to tree voxels.
Averaging these values across all cylinders, our ISA-guided
query point selection achieved a coverage rate of 92.7\% and
a tree voxel ratio of 98.5\% . In comparison, simple FPSbased sampling [43] yielded a coverage rate of 79.9\% and
a tree voxel ratio of 81.3\% . These results demonstrate that
our approach maximizes coverage of all tree instances while
minimizing the sampling of ground voxels.
Query decoder. The query decoder uses K_{\text {ins}} instance
query points selected by our ISA-guided method and K_{\text {sem}}
learnable semantic queries ( K_{\text {sem}} = 3 in our case). These
semantic queries are randomly initialized and updated during training to capture semantic information. The sparse
feature tensor \mathbf {F} \in \mathbb {R}^{M \times 32} serves as the key and value.
Following previous methods [30, 47], the decoder applies
six transformer layers with self-attention on the queries and
cross-attention with keys and values, outputting K_{\text {ins}} tree instance masks, K_{\text {sem}} semantic masks, and confidence score
from 0 to 1 for each predicted tree instance mask.
3.2. Training
One-to-many association. Our method explicitly selects
instance query points during the query point selection step,
meaning their spatial positions are inherently determined by
the selection process. Since these queries are pre-aligned
with tree instances at selection time, each predicted tree
instance mask can be directly associated with its corresponding ground truth (GT) instance, eliminating the need
for an additional matching step. In contrast, most existing
methods [30, 32, 43] require optimization-based matching
to establish correspondences between predicted masks and
GT instances. Inspired by one-to-many assignment strategies [71], we calculate score and mask losses for all predicted masks.
In previous one-to-one matching setups [30, 32, 43], the
goal is to ensure that each GT instance mask matches with
only one optimal predicted mask, enforcing distinct relationships among queries or masks and suppressing redundant masks. However, we observe that one-to-one matching, as in OneFormer3D, often produces numerous false
positives during inference, leading to lower final precision
for individual tree segmentation and still requiring an additional matrix-NMS (non-maximum suppression) step to
remove duplicate masks [30].
In contrast, our approach leverages a more controlled
instance query selection strategy, which aims to cover all
tree instances effectively, maximizing recall. With accurate
score predictions, our method quickly filters out duplicate
masks, thereby improving precision as well. This combination results in higher-quality individual tree predictions
overall. For semantic segmentation, we simplify the association process: given the three semantic masks, we directly
associate them with the GT masks in order.
Loss. The instance-related loss consists of binary crossentropy loss \mathcal {L}_{\text {bce}} , Dice loss \mathcal {L}_{\text {dice}} , and score loss \mathcal {L}_{\text {score}} [30].
The score loss is computed using mean squared error, with
the target score defined as the intersection-over-union (IoU)
between each predicted mask and the GT mask that has the
highest IoU with it. The total instance loss is:
 \vspace *{-3pt} \mathcal {L}_{\text {instance}} = \mathcal {L}_{\text {bce}} + \mathcal {L}_{\text {dice}} + 0.5 \cdot \mathcal {L}_{\text {score}}. \vspace *{-3pt} \label {eq:loss_instance} (1)
The semantic loss \mathcal {L}_{\text {sem}} is the cross-entropy loss [30].
Since our decoder consists of six layers, each layer‚Äôs output is compared with the GT to calculate the instance and
semantic losses, and these are summed across all layers:
 \vspace *{-3pt} \mathcal {L}_{\text {total}} = \sum _{\text {layer}=1}^{6} (\mathcal {L}_{\text {instance}} + 0.2 \cdot \mathcal {L}_{\text {sem}}), \vspace *{-1pt} \label {eq:loss_total} (2)
where the weights for loss terms are as in [30, 47]. Additionally, we include the losses from the two MLP heads
introduced in ISA-guided query point selection (Sec. 3.1):
the binary cross-entropy loss \mathcal {L}_{\text {binary}} , and the discriminative
loss \mathcal {L}_{\text {disc}} [17, 56, 62]. The final complete loss is thus:
 \vspace *{-3pt} \mathcal {L} = \mathcal {L}_{\text {total}} + \mathcal {L}_{\text {binary}} + \mathcal {L}_{\text {disc}}. \vspace *{-3pt} \label {eq:loss_final} (3)
3.3. Inference
Score-based block merging. In forest point clouds, cropping is unavoidable due to the large spatial extent, which
necessitates a merging strategy to handle overlapping regions (see Fig. 2). For semantic segmentation, merging is
relatively simple, as predictions can be resolved by voting
across overlapping regions. However, block merging strategies for instance segmentation remain limited and face several challenges.
Most existing instance segmentation tasks focus on
indoor datasets (where blocks often represent individual
rooms) or outdoor datasets for autonomous driving (where
blocks correspond to individual scans). In these settings,
block merging is unnecessary and therefore rarely studied.
In contrast, forest datasets cover vast areas, requiring a sliding window approach with fixed step sizes and overlapping
regions to manage computational constraints.
Existing block merging methods [18, 55] rely primarily on rule-based strategies, setting overlap thresholds to
determine whether instances in neighboring blocks should
merge. In our approach, we use a score-based method:
leveraging the scores obtained from the query decoder for
each predicted mask, we rank all masks across the scene and
apply NMS to remove lower-scoring, overlapping masks.
To further address issues caused by cylinder cropping
(which may split individual trees), inspired by the approach
of Henrich et al. [24], we discard masks near the crop
boundary. Specifically, for a cylinder radius of 16 m, we
remove any predicted instance whose mask includes even a
single point within 0.5 m of the cylinder boundary.
4. Experiments
4.1. Experimental settings
Dataset. We extend the FOR-instance dataset [41] by incorporating additional challenging datasets, resulting in a new,
comprehensive dataset named FOR-instanceV2. The newly
added datasets include NIBIO2, NIBIO MLS, BlueCat, and
Yuchen regions. These additions increase the dataset‚Äôs diversity and segmentation complexity by including Mobile
Laser Scanning (MLS) data (NIBIO MLS) collected via
backpack LiDAR, data from tropical regions (Yuchen), and
regions with highly varied tree sizes (NIBIO2 and BlueCat),
where both large and small trees coexist within the same
area. In addition, BlueCat (6.3k instances) and NIBIO2
(3k instances) are previously unavailable datasets, publicly released for the first time. They extend FOR-instance
(1.1k instances) eightfold number of trees. This expanded
dataset provides a more challenging benchmark for point
cloud segmentation in forests, and we plan to release FORinstanceV2 as an open dataset to support future research and
foster advancements in the field.
To assess transferability, we also include two additional
datasets, LAUTx and Wytham woods, which are used exclusively for testing and do not contain any training or validation data. Fig. 3 illustrates a geographical overview of
FOR-instanceV2 and these two test datasets. We provide
detailed statistics and annotation protocols in the supplementary materials.
Metrics. We adopt precision (Prec), recall (Rec), and F1-
score (F1) for individual tree segmentation. Predicted tree
instances are matched to GT trees based on the IoU of their
respective point sets, with predictions below an IoU threshold of 0.5 considered false positives. To further evaluate the
delineation accuracy of tree instances, we include a coverage metric (Cov) that quantifies the alignment between
predicted and GT boundaries [56, 63]. For each GT tree,
the predicted instance with the highest IoU is identified,
and coverage is calculated as the average IoU across all GT
trees. The mean intersection-over-union (mIoU) is used as
the semantic segmentation metric.
4.2. Comparison to baselines
All baseline methods and our proposed ForestFormer3D
were trained on the FOR-instanceV2 training set and evaluated on the corresponding test split, as shown in Tab. 1.
ForAINet [63] and TreeLearn [24] are two recent and effective deep learning-based methods for forest point cloud
segmentation. ForAINetV2 R8 maintains the original configuration of ForAINet with an input cylinder radius of
Figure 3. Geographical overview of the FOR-instanceV2 dataset
and the two additional test datasets, LAUTx and Wytham woods.
Different colored markers indicate the sensors used for data collection.
8 m, while ForAINetV2 R16 uses a 16 m radius to match
ForestFormer3D and isolate the effect of input size. TreeLearn [24], designed for ground-based LiDAR forest point
clouds, predicts offsets from individual points to their respective tree bases, defined as the trunk location three meters above ground. We adapted the training parameters of
TreeLearn for our experiments, reducing the epochs from
1400 to 1200, the learning rate from 0.002 to 0.001, and
the total number of samples from 25,000 to 2,500, while
increasing the batch size from 2 to 8 and adjusting the
chunk size from 35 to 30. While effective for regions
with dense stem points, this method struggles with ULS
data (e.g., NIBIO2, RMIT and Yuchen), where stem points
are sparse, showing its poor generalizability across diverse
sensor modalities. In particular, TreeLearn fails to predict
individual trees in the Yuchen region due to the extreme
sparsity of stem points. As a result, we compare TreeLearn‚Äôs performance only on regions excluding Yuchen in
the FOR-instanceV2 test split (as shown in Tab. 1). OneFormer3D [30], a state-of-the-art method in 3D point cloud
panoptic segmentation, is adapted for forest point cloud segmentation by employing the same block merging strategy as
ForAINet, using overlap thresholds to determine whether
instances in neighboring blocks should merge.
The results in Tab. 1 demonstrate that ForestFormer3D
outperforms all baselines, achieving the highest F1, surpassing the second-best method by 9.7 percent points (pp),
and the highest Cov, exceeding the next best by 1.2 pp. This
reflects ForestFormer3D‚Äôs superior ability to produce complete and accurate individual tree segmentations. While all
methods provide unique instance IDs for each point, the
high Prec of ForestFormer3D suggests a reduced likelihood
of over-segmentation compared to baselines.
Table 1. Comparison of individual tree segmentation and semantic segmentation results with baselines in FOR-instanceV2 test
split. The best results are in bold, and the second best ones are
underlined.
Method Individual Tree Seg. (%) Semantic Seg. (%)
Prec Rec F1 Cov Ground Wood Leaf mIoU
ForAINetV2 R8 84.3 63.4 72.4 73.3 98.3 68.1 94.3 86.9
ForAINetV2 R16 88.1 59.2 70.8 72.7 98.8 69.2 94.5 87.5
TreeLearn [24] 82.0 36.6 50.6 52.2 ‚Äì ‚Äì ‚Äì ‚Äì
OneFormer3D [30] 72.0 74.3 73.1 80.0 97.8 63.1 93.4 84.8
ForestFormer3D (ours) 92.4 75.0 82.8 81.2 96.9 67.7 94.1 86.2
To evaluate the transferability of our model, we test
on Wytham woods [8] and LAUTx [1] datasets, which
represent mixed-species and natural broadleaved temperate forests with diverse structural complexity. As shown
in Tab. 2, ForestFormer3D achieves best performance of
individual tree segmentation compared to previous methods evaluated on the Wytham woods dataset, achieving the
highest F1 and Cov. Although TreeLearn [24], a method
designed for ground-based LiDAR data, achieves the best
results on the LAUTx dataset, ForestFormer3D still ranks
second in F1 and Cov, and maintains competitive semantic
segmentation performance. These results demonstrate the
model‚Äôs good generalization ability to previously unseen
forest types. Notably, the Wytham woods region includes
tree species not present in the FOR-instanceV2 training set,
while the LAUTx dataset, collected using MLS, further validates the adaptability of ForestFormer3D across different
sensor modalities and forest structures.
Fig. 4 presents the qualitative results of ForestFormer3D
and OneFormer3D for individual tree segmentation across
different regions, demonstrating ForestFormer3D‚Äôs ability
to effectively mitigate over-segmentation and produce more
precise mask predictions.
Across datasets, our method achieves substantial F1 improvements for individual tree segmentation (+9.7 pp on
FOR-instanceV2 and +8.2 pp on Wytham woods), with
marginal mIoU reductions for semantic segmentation (-
1.3 pp and -1.0 pp, respectively). While mainly optimized
for individual tree segmentation, our framework also performs semantic segmentation within the same unified structure, leading to a slight reduction in mIoU, likely due
to shared representation learning prioritizing instance-level
distinctions. However, the overall semantic segmentation
performance remains stable.
4.3. Ablation studies
We conduct a series of ablation studies to analyze the impact
of the three proposed key components of ForestFormer3D:
Table 2. Comparison with previous methods on the individual tree
segmentation and semantic segmentation for Wytham woods and
LAUTx test data. The best results are in bold, and the second best
ones are underlined.
Region Method Individual Tree Seg. (%) Semantic Seg. (%)
Prec Rec F1 Cov Ground Tree mIoU
Wytham
woods
[8]
ForAINetV2 R16 82.2 53.8 65.1 54.4 84.2 99.8 92.0
SegmentAnyTree [58] 73.0 53.0 61.4 ‚Äì ‚Äì ‚Äì ‚Äì
TreeLearn [24] 52.5 91.9 66.8 49.2 ‚Äì ‚Äì ‚Äì
OneFormer3D [30] 41.4 62.4 49.7 61.6 88.8 99.9 94.4
ForestFormer3D
(ours)
81.8 69.2 75.0 66.9 86.9 99.9 93.4
LAUTx
[1]
Point2tree [57] 85.0 55.0 66.8 ‚Äì ‚Äì ‚Äì ‚Äì
TLS2trees [59] 70.0 40.5 51.3 ‚Äì ‚Äì ‚Äì ‚Äì
ForAINetV2 R16 93.7 77.7 85.0 74.7 95.9 96.7 96.3
SegmentAnyTree [58] 91.0 75.0 82.2 ‚Äì ‚Äì ‚Äì ‚Äì
TreeLearn [24] 96.8 87.9 92.1 85.2 92.6 95.9 94.3
OneFormer3D [30] 58.1 80.2 67.4 74.6 94.6 96.9 95.8
ForestFormer3D
(ours)
95.7 85.9 90.5 79.1 94.1 96.7 95.4
ISA-guided query point selection, the score-based block
merging strategy during inference, and one-to-many association during training (see Tab. 3). Here, we fix the score
threshold on mask removal to 0.6 for all methods. Additionally, we examine the effect of other factors on the performance of ForestFormer3D in the supplementary materials.
ISA-guided query point selection. As shown in Tab. 3,
the ISA-guided query point selection improves the F1 on
the FOR-instanceV2 test set by 3.4 pp and increases Prec
by 7.8 pp compared to the baseline OneFormer3D, which
uses randomly generated learnable query points. This
strategy ensures high-quality query points that are both
instance-discriminative (ensuring high individual tree coverage) and semantically aware (avoiding picking non-tree
points), thereby reducing the generation of false positive
masks. On the validation set, the ISA-guided query point
selection improves the F1 score by 0.9 pp and Prec by 4.4 pp
compared to the baseline.
Score-based block merging strategy. For the block merging strategy, two key designs contribute to the F1 improvement (see Tab. 3). First, replacing the overlap-based merging strategy of the baseline with a score-based approach
(BM1) to remove overlapping instances results in an F1 increase of 3.1 pp on the test split. When ISA-guided query
point selection and BM1 are combined, the F1 improves by
5.4 pp compared to the baseline. The second enhancement
involves discarding predicted masks near crop boundaries
to address inaccuracies caused by cropping (BM2). When
combined with ISA-guided query point selection, BM2 improves the F1 by an additional 2.2 pp over BM1. A similar
pattern can also be observed in the validation split.
One-to-many association. Changing the one-to-one
matching strategy in OneFormer3D to our one-to-many association yields a further 0.8 pp increase on both the FORinstanceV2 test and validation set (see Tab. 3).
Overall, incorporating all three key components results
in an 8.4 pp increase in F1, a 0.4 pp improvement in Cov,
Figure 4. Visual comparison of individual tree segmentation results between ForestFormer3D and OneFormer3D, with randomly assigned
colors representing different tree instances. Segmentation errors are highlighted using three distinct markers: yellow circles for oversegmentation, bright blue circles for undetected trees, and black circles for wrongly assigning points from one tree to a neighboring tree.
and a 1.5 pp increase in mIoU compared to the baseline in
test split. A similar pattern can also be observed in the validation split, except that the semantic segmentation mIoU
dropped by 0.5 pp compared to the baseline.
Table 3. Ablation study for three proposed key components on
the FOR-instanceV2 dataset. ISA represents ISA-guided query
point selection, BM1 indicates score-based block merging without
discarding masks near the crop boundary, BM2 uses score-based
block merging with discarding masks near the crop boundary, and
O2MA denotes one-to-many association. The best results are in
bold, and the second best ones are underlined.
ISA BM1 BM2 O2MA Individual Tree Seg. (%) Semantic Seg. (%)
Prec Rec F1 Cov mIoU
Test split
OneFormer3D, baseline 72.0 74.3 73.1 80.0 84.8
‚úì 80.2 73.1 76.5 80.0 84.1
‚úì 79.8 73.0 76.2 78.6 84.8
‚úì ‚úì 87.3 71.2 78.5 78.6 84.3
‚úì ‚úì 89.4 73.5 80.7 79.3 85.2
‚úì ‚úì ‚úì 93.9 71.9 81.5 80.4 86.3
Validation split
OneFormer3D, baseline 70.2 68.1 69.1 80.2 84.1
‚úì 74.6 66.0 70.0 79.4 82.4
‚úì 78.6 65.4 71.4 77.4 84.1
‚úì ‚úì 84.6 64.6 73.2 78.3 82.6
‚úì ‚úì 86.9 67.3 75.9 78.7 83.3
‚úì ‚úì ‚úì 93.2 65.1 76.7 80.5 83.6
5. Conclusion
We introduced ForestFormer3D, an end-to-end framework
designed for individual tree segmentation and semantic segmentation for forest 3D LiDAR point clouds. It incorporates three innovative components: ISA-guided query
point selection, a score-based block merging strategy for
inference, and one-to-many association during training.
ForestFormer3D achieves excellent individual tree segmentation performance on our newly introduced dataset (FORinstanceV2) and generalizes well to two additional unseen
datasets (Wytham woods and LAUTx). These results highlight its potential as a unified model, capable of effectively
handling point clouds collected from diverse forest types,
geographic regions, and sensor modalities without being
constrained by specific conditions.
We hope this work inspires research into segmentation
methods tailored for forest environments and encourages
the community to experiment and advance algorithms using the FOR-instanceV2 dataset. ForestFormer3D aims to
serve as a strong baseline, facilitating further developments
and practical applications in forestry analysis, environmental monitoring, and sustainable resource management.
Acknowledgements
This work is part of the Center for Research-based Innovation SmartForest: Bringing Industry 4.0 to the Norwegian forest sector (NFR SFI project no. 309671, smartforest.no). MK, KK, and AM were funded by the INTERCOST project LUC23023.
ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest
LiDAR 3D Point Clouds
Supplementary Material
1. Overview
In this supplementary material, we provide:
‚Ä¢ implementation details (Sec. 2),
‚Ä¢ additional dataset details and point cloud annotation
(Sec. 3),
‚Ä¢ a quantitative comparison including both individual tree
segmentation and semantic segmentation across nine regions in the FOR-instanceV2 test split (Sec. 4),
‚Ä¢ additional ablation studies (Sec. 5),
‚Ä¢ qualitative results (Sec. 6), and
‚Ä¢ relation to forest domain specific metrics (Sec. 7).
2. Implementation details
3D sparse U-Net architecture. The 3D sparse U-Net used
for sparse tensor feature extraction is shown in Fig. 5.
Loss calculation. All instance masks, predicted by transformer decoder, are supervised by losses including binary
cross-entropy (BCE), Dice, and score loss.
 \mathcal {L}_{\text {bce}} = - \frac {1}{N} \sum _{i=1}^{N} \left ( y_i \log \sigma (\hat {y}_i) + (1 - y_i) \log (1 - \sigma (\hat {y}_i)) \right ), \label {eq:loss_bce}
(4)
 \mathcal {L}_{\text {dice}} = 1 - \frac {2 \sum _{i=1}^{N} \sigma (\hat {y}_i) y_i + 1}{\sum _{i=1}^{N} \sigma (\hat {y}_i) + \sum _{i=1}^{N} y_i + 1}, \label {eq:loss_dice} (5)
where N is the number of voxels, \hat {y}_i is the predicted
mask logits at voxel i , y_i \in \{0,1\} is the GT mask value,
and \sigma (\cdot ) represents the sigmoid function. The additive constant +1 in Eq. (5) prevents division by zero.
The score loss supervises the predicted confidence scores
of all predicted instance masks using mean squared error:
 \mathcal {L}_{\text {score}} = \frac {1}{M} \sum _{j=1}^{M} (\hat {s}_j - s_j)^2, \label {eq:loss_score} (6)
where M is the number of predicted masks, \hat {s}_j is the
predicted score for predicted mask j , and s_j is set as the
IoU between the predicted mask and its best matched GT
mask, or zero if no GT overlaps. IoU is computed based on
the number of intersecting voxels divided by the union of
the predicted and GT mask voxels.
To facilitate query point selection, we first construct a 5D
embedding space where voxels from the same instance are
close while those from different instances are apart. This
embedding is learned using discriminative loss:
 \mathcal {L}_{\text {disc}} = \mathcal {L}_{\text {var}} + \mathcal {L}_{\text {dist}} + 0.001 \cdot \mathcal {L}_{\text {reg}}. \label {eq:loss_disc} (7)
 \mathcal {L}_{\text {var}} encourages intra-instance compactness by pulling
voxel embeddings within the same instance toward their
mean. \mathcal {L}_{\text {dist}} enforces inter-instance separation, and \mathcal {L}_{\text {reg}} constrains instance centroids to stay near the origin. Let C denote the total number of tree instances, and \mathcal {I}_c the set of
voxels belonging to instance c . Each voxel i has an embedding {f}_i , and the mean embedding of instance c is \mu _c .
We define N_c as the number of voxels in instance c . The
margins \delta _v and \delta _d control the desired intra-instance compactness and inter-instance separation, respectively. Three
loss terms are calculated by:
 \mathcal {L}_{\text {var}} = \frac {1}{C} \sum _{c=1}^{C} \frac {1}{N_c} \sum _{i \in \mathcal {I}_c} \left [ \max \bigl (\|f_i - \mu _c\|_1 - \delta _v, 0\bigr ) \right ]^2, \label {eq:loss_var_new} (8)
 \mathcal {L}_{\text {dist}} = \frac {1}{C(C-1)} \sum _{c_1 \neq c_2} \left [ \max \bigl ( 2\delta _d - \|\mu _{c_1} - \mu _{c_2}\|_1, 0 \bigr ) \right ]^2, \label {eq:loss_dist_new}
(9)
 \mathcal {L}_{\text {reg}} = \frac {1}{C}\sum _{c=1}^{C} \|\mu _c\|_1. \label {eq:loss_reg_new} (10)
We set \delta _v = 0.5 and \delta _d = 1.5 following prior work [17,
56, 62].
In addition, to ensure that query points are sampled from
tree voxels, we train a tree and non-tree classification head
to distinguish tree and non-tree voxels using the BCE loss:
 \mathcal {L}_{\text {binary}} = - \frac {1}{N} \sum _{i=1}^{N} ( l_i \log \hat {l}_i + (1 - l_i) \log (1 - \hat {l}_i) ), \label {eq:loss_binary} (11)
where l_i is the GT label for voxel i, and \protect \hat {l}_i is the predicted
class probability.
Other implementation details. Point clouds are voxelized
to a resolution of 0.2 m. Data augmentation includes random horizontal flipping, random rotation around the Z-axis,
and random scaling by a factor between 0.8 and 1.2. We
train our model on a single NVIDIA A100 GPU with 80
GB memory, using a batch size of 2 for 3000 epochs.
To stabilize training, we first pretrain two MLP heads
introduced in ISA-guided query point selection for 1000
epochs before jointly training the full model. Specifically,
the 5D embedding feature head is trained with the discriminative loss (see Eq. (7)) to enforce compact intra-instance
representations and inter-instance separation, while the binary classification head is trained with a binary crossentropy loss to distinguish tree voxels from non-tree voxels
(see Eq. (11)). During this warm-up stage, the decoder remains frozen, ensuring that the MLP heads produce stable
embeddings and classifications before integrating with the
full network. After this stage, we jointly train all components, including the 3D sparse U-Net and the transformer
decoder, optimizing all loss terms together.
For inference, we sample 300 query points within each
cylindrical region of radius 16 m with a stride of 4 m using a
sliding window approach. The best score threshold on mask
removal is 0.4 according to our ablation studies (Fig. 7(d)).
3. Dataset details and point cloud annotation
Tab. 4 summarizes the key characteristics of the FORinstanceV2 dataset and the additional test datasets.
For the BlueCat data annotation, all trees were manually labeled using 3D Forest software [49] by two independent annotators working on separate, non-overlapping
datasets. After labeling, the trees were matched to field
measurements based on spatial proximity. This pairing was
validated by comparing tree heights derived from the point
cloud with those estimated from DBH-based allometry. In
addition, every tree with a DBH of 1 cm or greater was fieldverified to eliminate false positives, and any obvious labeling errors were manually refined.
For the NIBIO2 data, annotations were performed using
CloudCompare software (https://www.cloudcompare.org,
V 2.12.4) by two annotators, and the results were later reviewed to ensure consistency and accuracy.
4. Comparison to baselines across nine regions
in the FOR-instanceV2 test split
We evaluate ForestFormer3D‚Äôs quantitative performance
across nine different forest regions in the FOR-instanceV2
test set and compare it against other methods (see Tab. 5).
It is important to note that CHM-based YOLOv5 [45],
ForAINet [63], and SegmentAnyTree [58] are trained on the
original FOR-instance dataset [41], which does not include
data from the NIBIO MLS, BlueCat, and Yuchen regions.
As shown in Tab. 5, ForestFormer3D consistently achieves
the best results for individual tree segmentation across all
nine regions except for SCION. In simpler forest areas,
such as CULS, NIBIO, SCION, and NIBIO MLS, ForestFormer3D maintains high F1 and Cov scores. In more challenging environments, such as TUWIEN (where trees are
closely intertwined), RMIT and Yuchen (with sparse point
densities), and NIBIO2 and BlueCat (characterized by both
large canopy trees and closely packed understory trees),
ForestFormer3D surpasses all prior baselines. It demonstrates substantial improvements in regions that were previously difficult for existing methods. For example, in the
RMIT region, ForestFormer3D improves the F1 by 6.8 pp
over the second best one. In the SCION region, the Cov
metric shows an increase of 5.1 pp. The NIBIO2 region is
particularly challenging due to the presence of both understory and canopy trees, as noted in prior work [63]. Despite
this, ForestFormer3D outperforms the OneFormer3D baseline by 8.2 pp in F1 and achieves a 17.6 pp improvement
over the reported F1 of 72.8% from ForAINet [63].
For semantic segmentation, ForestFormer3D demonstrates competitive or superior performance in most regions,
with slight decreases in mIoU observed only in the CULS
and NIBIO MLS regions compared to the best-performing
method.
5. Additional ablation studies
Input radius size and number of query points. Fig. 6
compares the effect of different cylinder input radii and
the number of query points on individual tree segmentation metrics (F1 and Cov) within the FOR-instanceV2 test
split. Fig. 6(a) shows the performance of OneFormer3D,
while Fig. 6(b) shows the performance of ForestFormer3D.
r8 qp200 represents a cylinder radius of 8 m with 200
query points, with similar interpretations for other settings. For ForestFormer3D, increasing the input radius and
the number of query points generally results in higher F1
scores. However, due to GPU memory limitations, further
increases in these parameters were not explored. For OneFormer3D, achieving an optimal balance between the input
radius and the number of query points is crucial to better
performance.
Score threshold on mask removal. We evaluate the effect of the score threshold on Prec, Rec, F1, and Cov using
the FOR-instanceV2 dataset (see Fig. 7). During inference,
masks with scores below the threshold are removed as unreliable, affecting the balance between retaining true positives
and removing false positives. As shown in Fig. 7, higher
thresholds improve Prec (Fig. 7(b)) by reducing false positives but decrease Cov (Fig. 7(a)) and Rec (Fig. 7(c)) by
discarding true positives. Fig. 7(d) highlights the trade-off
between Prec and Rec. A threshold of 0.4 achieves the best
F1 score in both validation and test sets, making it the optimal choice for our framework.
Point cloud density. We evaluated the impact of different point cloud densities on segmentation performance.
The original FOR-instanceV2 dataset was downsampled to
seven densities: 10, 25, 50, 75, 100, 500, and 1000 points
per square meter (pts/m¬≤). The results are shown in Fig. 8.
As the point density decreases, all metrics experience varying degrees of degradation. In particular, mIoU and Prec remain relatively stable in different densities, while Rec, Cov,
3 I
Input
3232 I
64 64 I/2
96 96 I/4
128 128 I/8
160 160 I/16
128 128
96 96
64 64
3232
Output
Input layer
Residual block
Downsample
Upsample
Skip connection
Figure 5. The detailed structure of the 3D sparse U-Net architecture used for feature extraction from the voxelized 3D point cloud.
Table 4. Characteristics of the FOR-instanceV2 dataset and additional test datasets in different geographic regions. The FOR-instanceV2
dataset contains unique individual tree IDs for each point, as well as semantic labels for ground, wood, and leaf. The additional test data,
on the other hand, includes only tree and non-tree labels, along with individual tree IDs for each point.
Region name Forest type (Tree species) Scanning mode (Sensor) Number of plots Number
of trees Country
Train Val Test
FOR-instanceV2
CULS [41] Coniferous dominated temperate forest (Pinus sylvestris) ULS (Riegl VUX-1) 1 1 1 37 Czech Republic
NIBIO [41] Coniferous dominated boreal forest (Picea abies, Pinus
sylvestris, Betula sp. (few))
ULS (Riegl MiniVUX-1) 8 6 6 575 Norway
RMIT [41] Native dry sclerophyll eucalypt forest (Eucalyptus sp.) ULS (Riegl MiniVUX-1) 1 0 1 223 Australia
SCION [41] Non-native pure coniferous temperate forest (Pinus radiata) ULS (Riegl MiniVUX-1) 2 1 2 135 New Zealand
TUWIEN [41] Deciduous dominated temperate forest (Deciduous species) ULS (Riegl VUX-1) 1 0 1 150 Austria
NIBIO2 Coniferous dominated boreal forest (Pinus sylvestris, Picea
abies, Betula sp.)
ULS (Riegl VUX-1) 29 6 15 3062 Norway
NIBIO MLS
[58]
Coniferous dominated boreal forest (Picea abies, Pinus
sylvestris, Betula sp.)
MLS (GeoSLAM ZEBHORIZON)
4 1 1 258 Norway
BlueCat Deciduous temperate forest (Mixed species) TLS (Leica P20) 1 1 1 6304 Czech Republic
Yuchen [5] Tropical forest ULS (Riegl MiniVUX-1) 1 1 1 281 French Guiana
Additional
Wytham
woods [8]
Deciduous temperate forest (Fraxinus excelsior, Acer pseudoplatanus, Corylus avellana)
TLS (RIEGL VZ-400) 0 0 1 835 United Kingdom
LAUTx [1] Mixed temperate broad leaved, coniferous forest (Mixed
species)
MLS (GeoSLAM ZEBHORIZON)
0 0 6 516 Austria
Figure 6. Impact of cylinder input radius and number of
query points on individual tree segmentation metrics for FORinstanceV2 test split.
and F1 show more noticeable drops when the density falls
below 500 pts/m¬≤. This suggests that the method may face
certain limitations in extremely sparse point cloud scenarios.
To improve robustness under varying point cloud densities, we retrained ForestFormer3D using the multi density augmentation strategy from SegmentAnyTree [58]. The
training set combined point clouds from eight densities (i.e.,
the original resolution and seven downsampled versions).
As shown in Fig. 9, this strategy improves individual tree
segmentation under sparse conditions (e.g., < 100 pts/m¬≤),
yielding higher F1 scores. However, it brings no benefit
for dense inputs and can even degrade performance. These
results suggest that simple multi-density training is insufficient to address the challenges posed by point density variation, which remains an open problem.
Table 5. Comparison with other baselines on the individual tree segmentation and semantic segmentation for different forest regions in
FOR-instanceV2 test split. The best results are in bold, and the second best ones are underlined.
Region Method Individual Tree Seg. (%) Semantic Seg. (%)
Prec Rec F1 Cov Ground Wood Leaf mIoU
CULS
CHM-based YOLOv5 [45] 100.0 100.0 100.0 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì
ForAINet [63] 87.0 100.0 93.0 98.2 ‚Äì ‚Äì ‚Äì ‚Äì
SegmentAnyTree [58] 100.0 100.0 100.0 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì
ForAINetV2 R16 100.0 100.0 100.0 96.6 99.8 67.2 96.4 87.8
OneFormer3D [30] 95.0 95.0 95.0 94.6 99.8 69.0 96.5 88.5
ForestFormer3D (ours) 100.0 100.0 100.0 99.4 99.8 66.1 96.2 87.4
NIBIO
CHM-based YOLOv5 [45] 87.0 72.0 79.0 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì
ForAINet [63] 96.4 88.4 92.4 79.4 ‚Äì ‚Äì ‚Äì ‚Äì
SegmentAnyTree [58] 91.0 88.0 89.5 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì
ForAINetV2 R16 98.1 89.4 93.4 82.4 97.9 62.1 93.3 84.4
OneFormer3D [30] 79.2 96.2 86.6 88.9 97.8 60.7 93.0 83.9
ForestFormer3D (ours) 97.7 95.8 96.7 88.8 98.1 64.2 93.5 85.2
RMIT
CHM-based YOLOv5 [45] 70.0 62.0 65.0 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì
ForAINet [63] 75.9 64.1 69.5 60.6 ‚Äì ‚Äì ‚Äì ‚Äì
SegmentAnyTree [58] 83.0 69.0 75.4 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì
ForAINetV2 R16 74.5 59.4 66.1 60.3 98.3 55.8 92.7 82.3
OneFormer3D [30] 70.3 81.2 75.4 73.7 98.1 52.7 92.8 81.2
ForestFormer3D (ours) 81.5 82.8 82.2 73.5 98.2 58.1 92.7 83.0
SCION
CHM-based YOLOv5 [45] 91.0 91.0 91.0 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì
ForAINet [63] 96.0 87.7 91.5 83.1 ‚Äì ‚Äì ‚Äì ‚Äì
SegmentAnyTree [58] 93.0 92.0 92.5 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì
ForAINetV2 R16 100.0 90.4 95.0 83.2 99.7 61.9 95.1 85.6
OneFormer3D [30] 58.7 92.4 71.7 83.6 99.7 61.7 95.0 85.5
ForestFormer3D (ours) 97.1 92.4 94.7 88.7 99.7 64.4 95.5 86.5
TUWIEN
CHM-based YOLOv5 [45] 41.0 23.0 30.0 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì
ForAINet [63] 66.6 71.4 69.4 58.3 ‚Äì ‚Äì ‚Äì ‚Äì
SegmentAnyTree [58] 55.0 46.0 50.1 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì
ForAINetV2 R16 68.2 42.9 52.6 47.9 98.2 53.3 94.5 82.0
OneFormer3D [30] 41.5 62.9 50.0 54.8 98.4 48.4 94.2 80.3
ForestFormer3D (ours) 92.0 65.7 76.7 54.8 98.5 52.8 94.2 81.8
NIBIO2
ForAINet [63] 83.5 64.5 72.8 ‚Äì ‚Äì ‚Äì ‚Äì
ForAINetV2 R16 91.8 72.9 80.6 70.0 96.5 52.9 95.7 81.7
OneFormer3D [30] 79.2 85.9 82.2 79.1 96.9 54.4 95.8 82.3
ForestFormer3D (ours) 94.6 86.9 90.4 80.2 96.9 56.6 95.9 83.1
NIBIO MLS
ForAINetV2 R16 95.5 91.3 93.3 84.4 98.8 74.9 86.4 86.7
OneFormer3D [30] 79.3 100.0 88.5 89.8 98.9 75.3 87.2 87.1
ForestFormer3D (ours) 100.0 91.3 95.5 85.4 94.5 77.6 87.1 86.4
BlueCat
ForAINetV2 R16 71.8 27.9 40.2 32.8 ‚Äì 73.3 94.7 84.0
OneFormer3D [30] 59.7 47.1 52.6 48.3 ‚Äì 64.4 93.0 78.7
ForestFormer3D (ours) 84.5 48.6 61.7 48.8 ‚Äì 69.9 93.9 81.9
Yuchen
ForAINetV2 R16 78.3 75.0 76.6 74.9 99.4 51.7 98.1 83.0
OneFormer3D [30] 52.9 75.0 62.1 71.6 99.6 50.0 97.9 82.5
ForestFormer3D (ours) 90.5 79.2 84.4 79.2 99.7 49.7 98.1 82.5
6. Qualitative results
To provide an intuitive understanding of how segmentation
metrics relate to actual segmentation quality, we present visualizations of ForestFormer3D‚Äôs results in individual tree
segmentation and semantic segmentation tasks.
Fig. 10 shows both individual tree segmentation and semantic segmentation predictions, along with corresponding
ground truth segmentations, across nine different forest regions in the FOR-instanceV2 test split. The results show
that our ForestFormer3D performs well under various conditions, including data collected from different sensors, different point densities, varying forest types and geographical
locations (i.e., boreal, temperate and tropical forests), and
complex environments where both large and small trees coexist. These visual results demonstrate the robustness of
ForestFormer3D in handling diverse and challenging forest
scenarios.
Fig. 11 displays individual tree segmentation results
with predicted and ground truth segmentations for ForestFormer3D on the Wytham woods and LAUTx datasets.
Fig. 12 presents three representative examples, progressing
from simple forest scenes to densely packed environments
with both large canopy trees and understory ones. For each
example, we include visualizations of the ISA-guided query
points, the predicted masks generated from these points,
and the final individual tree segmentation predictions, along
with the ground truth for comparison.
Fig. 13 presents a visual comparison of ForestFormer3D
Figure 7. Effect of score threshold on key metrics of individual
tree segmentation (Prec, Rec, F1 and Cov) for FOR-instanceV2
dataset.
Figure 8. Effect of point density on metrics of individual tree
segmentation (Prec, Rec, F1 and Cov) and semantic segmentation
(mIoU) for FOR-instanceV2 test split.
and baseline methods. In Example 1 and Example 2,
ForestFormer3D demonstrates its ability to effectively mitigate over-segmentation and under-segmentation compared
to other methods. However, in Example 2, small trees
are occasionally missed, indicating a potential limitation
in detecting such cases. In Example 3, heavily inclined
small trees near the ground pose significant challenges
for all methods, leading to either missed detections, oversegmentation, or incorrect assignment to neighboring larger
trees. Addressing these complex scenarios remains an open
problem for future research.
Figure 9. Effect of multi density augmentation on individual
tree segmentation (F1) and semantic segmentation (mIoU) on the
FOR-instanceV2 test split.
7. Relation to forest domain specific metrics
While domain-specific metrics have long been used in
forestry research, we adopt Prec, Rec, and F1 to align
with standard evaluation practices in computer vision.
This allows for direct comparability across domains while
maintaining consistency with traditional forestry metrics.
Specifically, completeness corresponds to Rec, omission error to 1 - \text {Rec} , commission error to 1 - \text {Prec} , and F-score remains equivalent to F1. These relationships have been previously established in forestry research [63], ensuring that
our reported results remain interpretable for both forestry
and computer vision communities.
It is worth noting that SegmentAnyTree [58] reports locally computed F1 scores, whereas we adopt global metrics
consistent with ForAINet [63]. This difference in evaluation metrics explains the discrepancy in reported F1 scores
across methods.
References
[1] Tockner Andreas, Gollob Christoph, Ritter Tim, and Nothdurft Arne. LAUTx - individual tree point clouds from austrian forest inventory plots, 2022. 3, 7
[2] Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3D semantic parsing of large-scale indoor spaces. In CVPR, pages
1534‚Äì1543, 2016. 2
[3] Iro Armeni, Sasha Sax, Amir R. Zamir, and Silvio Savarese.
Joint 2D-3D-Semantic data for indoor scene understanding.
arXiv preprint arXiv:1702.01105, 2017. 3
[4] Elias Ayrey, Shawn Fraver, John A. Kershaw Jr., Laura S.
Kenefic, Daniel Hayes, Aaron R. Weiskittel, and Brian E.
Roth. Layer stacking: A novel algorithm for individual forest tree segmentation from LiDAR point clouds. Canadian
Journal of Remote Sensing, 43(1):16‚Äì27, 2017. 3
[5] Yuchen Bai, Jean-Baptiste Durand, Gregoire Vincent, and ¬¥
Florence Forbes. Semantic segmentation of sparse irregular
point clouds for leaf/wood discrimination. In NIPS, pages
48293‚Äì48313, 2023. 3
Figure 10. Visualization of ForestFormer3D segmentation results for nine different regions in FOR-instanceV2 test split. The colors for
the individual trees are assigned randomly.
[6] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Se- ¬®
manticKITTI: A dataset for semantic scene understanding of
LiDAR sequences. In ICCV, pages 9296‚Äì9306, 2019. 2
Figure 11. Visualization of ForestFormer3D individual tree segmentation results for Wytham woods and LAUTx. The colors for the
individual trees are assigned randomly.
[7] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,
Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In CVPR, pages
11618‚Äì11628, 2020. 2
[8] Kim Calders, Hans Verbeeck, Andrew Burt, Niall Origo,
Joanne Nightingale, Yadvinder Malhi, Phil Wilkes, Pasi Raumonen, Robert GH Bunce, and Mathias Disney. Laser scanning reveals potential underestimation of biomass carbon in
temperate forest. Ecological Solutions and Evidence, 3(4):
e12197, 2022. 3, 7
[9] Lihong Chang, Hongchao Fan, Ningning Zhu, and Zhen
Dong. A two-stage approach for individual tree segmentation from TLS point clouds. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 15:
8682‚Äì8693, 2022. 3
[10] R. Qi Charles, Hao Su, Mo Kaichun, and Leonidas J. Guibas.
PointNet: Deep learning on point sets for 3D classification
and segmentation. In CVPR, pages 77‚Äì85, 2017. 2
[11] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and
Xinggang Wang. Hierarchical aggregation for 3D instance
segmentation. In ICCV, pages 15447‚Äì15456, 2021. 2
[12] Xinxin Chen, Kang Jiang, Yushi Zhu, Xiangjun Wang, and
Ting Yun. Individual tree crown segmentation directly from
UAV-borne LiDAR data using the PointNet of deep learning.
Forests, 12(2):131, 2021. 3
[13] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4D
spatio-temporal convnets: Minkowski convolutional neural
networks. In CVPR, pages 3075‚Äì3084, 2019. 2, 3
[14] Spconv Contributors. Spconv: Spatially sparse convolution library. https://github.com/traveller59/
spconv, 2022. 4
[15] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie√üner. ScanNet:
Richly-annotated 3D reconstructions of indoor scenes. In
CVPR, pages 5828‚Äì5839, 2017. 2, 3
[16] Michele Dalponte and David A. Coomes. Tree-centric mapping of forest carbon density from airborne laser scanning
and hyperspectral data. Methods in Ecology and Evolution,
7(10):1236‚Äì1245, 2016. 3
[17] Bert De Brabandere, Davy Neven, and Luc Van Gool. Semantic instance segmentation for autonomous driving. In
CVPRW, pages 478‚Äì480, 2017. 4, 5, 1
[18] Leon Denis, Remco Royen, and Adrian Munteanu. Improved
block merging for 3D point cloud instance segmentation. In
2023 24th International Conference on Digital Signal Processing (DSP), pages 1‚Äì5, 2023. 5
[19] Cathrin Elich, Francis Engelmann, Theodora Kontogianni,
and Bastian Leibe. 3D Bird‚Äôs-Eye-View instance segmentation. In PR, pages 48‚Äì61, 2019. 2
[20] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian
Leibe, and Matthias Nie√üner. 3D-MPA: Multi proposal aggregation for 3D semantic instance segmentation. In CVPR,
pages 9031‚Äì9040, 2020. 2
[21] Antonio Ferraz, Fr ¬¥ ed¬¥ eric Bretar, St ¬¥ ephane Jacquemoud, ¬¥
Gil Gonc¬∏alves, Lu¬¥ƒ±sa Pereira, Margarida Tome, and Paula ¬¥
Soares. 3-D mapping of a multi-layered mediterranean forest using ALS data. Remote Sensing of Environment, 121:
210‚Äì223, 2012. 3
[22] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg:
Occupancy-aware 3D instance segmentation. In CVPR,
pages 2937‚Äì2946, 2020. 2
[23] Junjie He, Pengyu Li, Yifeng Geng, and Xuansong Xie.
FastInst: A simple query-based model for real-time instance
segmentation. In CVPR, pages 23663‚Äì23672, 2023. 4
[24] Jonathan Henrich, Jan van Delden, Dominik Seidel, Thomas
Kneib, and Alexander Ecker. TreeLearn: A comprehensive
Figure 12. Visual comparison of individual tree segmentation for three representative forest scenes, with each row illustrating a different
stage in our segmentation framework. In the first row, the selected ISA-guided query points are highlighted as enlarged, randomly colored
points, effectively covering nearly every tree in the scene. The second row shows the predicted masks generated based on the query
points from the first row, with each mask color corresponding to its associated query point color. The third row increases color contrast
to distinguish each predicted individual tree, with colors again randomly assigned. The fourth row presents the ground truth segmentation
with randomly assigned colors to aid comparison.
deep learning method for segmenting individual trees from
ground-based LiDAR forest point clouds. arXiv preprint
arXiv:2309.08471, 2024. 3, 5, 6, 7
[25] J. Hyyppa, O. Kelle, M. Lehikoinen, and M. Inkinen. A
segmentation-based method to retrieve stem volume estimates from 3-D tree height models produced by laser scanners. IEEE Transactions on Geoscience and Remote Sensing,
39(5):969‚Äì975, 2001. 3
Figure 13. Visual comparison of individual tree segmentation results among ForestFormer3D and baseline methods, with randomly assigned colors representing different tree instances. Common segmentation errors are highlighted using four distinct markers: red circles
for under-segmentation, yellow circles for over-segmentation, bright blue circles for undetected trees, and black circles for other errors,
such as points from one tree being assigned to a neighboring tree.
[26] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi- Wing Fu, and Jiaya Jia. PointGroup: Dual-set point grouping
for 3D instance segmentation. In CVPR, pages 4866‚Äì4875,
2020. 2
[27] Tengping Jiang, Qinyu Zhang, Shan Liu, Chong Liang, Lei
Dai, Zequn Zhang, Jian Sun, and Yongjun Wang. LWSNet:
A point-based segmentation network for leaf-wood separation of individual trees. Forests, 14(7):1303, 2023. 3
[28] Dong-Hyeon Kim, Chi-Ung Ko, Dong-Geun Kim, Jin-Taek
Kang, Jeong-Mook Park, and Hyung-Ju Cho. Automated
segmentation of individual tree structures using deep learning over LiDAR point cloud data. Forests, 14(6):1159, 2023.
3
[29] Maksim Kolodiazhnyi, Anna Vorontsova, Anton Konushin,
and Danila Rukhovich. Top-down beats bottom-up in 3D instance segmentation. In 2024 IEEE/CVF Winter Conference
on Applications of Computer Vision (WACV), pages 3554‚Äì
3562, 2024. 2
[30] Maxim Kolodiazhnyi, Anna Vorontsova, Anton Konushin,
and Danila Rukhovich. OneFormer3D: One transformer for
unified point cloud segmentation. In CVPR, pages 20943‚Äì
20953, 2024. 2, 3, 4, 5, 6, 7
[31] Sean Krisanski, Mohammad Sadegh Taskhiri, Susana Gonzalez Aracil, David Herries, and Paul Turner. Sensor agnostic semantic segmentation of structurally diverse and complex forest point clouds using deep learning. Remote Sensing, 13(8):1413, 2021. 3
[32] Harold W Kuhn. The Hungarian method for the assignment
problem. Naval research logistics quarterly, 2(1-2):83‚Äì97,
1955. 3, 5
[33] Chen Liu and Yasutaka Furukawa. MASC: Multi-scale affinity with sparse convolution for 3D instance segmentation.
arXiv preprint arXiv:1902.04478, 2019. 2
[34] Haijian Liu, Pinliang Dong, Changshan Wu, Pin Wang, and
Meihong Fang. Individual tree identification using a new
cluster-based approach with discrete-return airborne LiDAR
data. Remote Sensing of Environment, 258:112382, 2021. 3
[35] Jiaheng Liu, Tong He, Honghui Yang, Rui Su, Jiayi Tian,
Junran Wu, Hongcheng Guo, Ke Xu, and Wanli Ouyang.
3D-QueryIS: A query-based framework for 3D instance segmentation. arXiv preprint arXiv:2211.09375, 2022. 3, 4
[36] Romain Loiseau, Elliot Vincent, Mathieu Aubry, and Loic
Landrieu. Learnable earth parser: Discovering 3D prototypes in aerial scans. In CVPR, pages 27874‚Äì27884, 2024.
3
[37] Jiahao Lu, Jiacheng Deng, Chuxin Wang, Jianfeng He, and
Tianzhu Zhang. Query refinement transformer for 3D instance segmentation. In ICCV, pages 18470‚Äì18480, 2023.
2, 3, 4
[38] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng,
Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang.
Conditional DETR for fast training convergence. In ICCV,
pages 3631‚Äì3640, 2021. 4
[39] Quang-Hieu Pham, Thanh Nguyen, Binh-Son Hua, Gemma
Roig, and Sai-Kit Yeung. JSIS3D: Joint semantic-instance
segmentation of 3D point clouds with multi-task pointwise
networks and multi-value conditional random fields. In
CVPR, pages 8819‚Äì8828, 2019. 2
[40] Sorin C Popescu, Randolph H Wynne, and Ross F Nelson.
Estimating plot-level tree heights with lidar: local filtering
with a canopy-height based variable window size. Computers and Electronics in Agriculture, 37(1):71‚Äì95, 2002. 3
[41] Stefano Puliti, Grant Pearse, Peter Surovy, Luke Wallace, ¬¥
Markus Hollaus, Maciej Wielgosz, and Rasmus Astrup.
FOR-instance: a UAV laser scanning benchmark dataset
for semantic and instance segmentation of individual trees.
arXiv preprint arXiv:2309.01279, 2023. 1, 3, 6, 2
[42] Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. PointNet++: Deep hierarchical feature learning on point sets in a
metric space. In NIPS, pages 5105‚Äì5114, 2017. 2
[43] Jonas Schult, Francis Engelmann, Alexander Hermans, Or
Litany, Siyu Tang, and Bastian Leibe. Mask3D: Mask transformer for 3D semantic instance segmentation. In 2023
IEEE International Conference on Robotics and Automation
(ICRA), pages 8216‚Äì8223, 2023. 2, 3, 4, 5
[44] Sangyun Shin, Kaichen Zhou, Madhu Vankadari, Andrew
Markham, and Niki Trigoni. Spherical mask: Coarse-to-fine
3D point cloud instance segmentation with spherical representation. In CVPR, pages 4060‚Äì4069, 2024. 3
[45] Adrian Straker, Stefano Puliti, Johannes Breidenbach,
Christoph Kleinn, Grant Pearse, Rasmus Astrup, and Paul
Magdon. Instance segmentation of individual tree crowns
with YOLOv5: A comparison of approaches using the ForInstance benchmark LiDAR dataset. ISPRS Open Journal of
Photogrammetry and Remote Sensing, 9:100045, 2023. 3, 2,
4
[46] Chenxin Sun, Chengwei Huang, Huaiqing Zhang, Bangqian
Chen, Feng An, Liwen Wang, and Ting Yun. Individual
tree crown segmentation and crown width extraction from
a heightmap derived from aerial laser scanning data using
a deep learning framework. Frontiers in Plant Science, 13:
914974, 2022. 3
[47] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu.
Superpoint transformer for 3D scene instance segmentation.
In AAAI, 2023. 2, 5
[48] Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud,
Beatriz Marcotegui, Franc¬∏ois Goulette, and Leonidas
Guibas. KPConv: Flexible and deformable convolution for
point clouds. In ICCV, pages 6410‚Äì6419, 2019. 2
[49] Jan Trochta, Martin KruÀöcek, Tom Àá a¬¥s Vr Àá ska, and Kamil Àá
Kral. 3D Forest: An application for descriptions of three- ¬¥
dimensional forest structures using terrestrial LiDAR. PloS
one, 12(5):e0176871, 2017. 2
[50] Matheus B. Vicari, Mathias Disney, Phil Wilkes, Andrew
Burt, Kim Calders, and William Woodgate. Leaf and wood
classification framework for terrestrial lidar point clouds.
Methods in Ecology and Evolution, 10(5):680‚Äì694, 2019. 3
[51] Thang Vu, Kookhoi Kim, Tung M. Luu, Thanh Nguyen, and
Chang D. Yoo. SoftGroup for 3D instance segmentation on
point clouds. In CVPR, pages 2698‚Äì2707, 2022. 2
[52] Peng Wan, Jie Shao, Shuangna Jin, Tiejun Wang, Shengmei
Yang, Guangjian Yan, and Wuming Zhang. A novel and efficient method for wood‚Äìleaf separation from terrestrial laser
scanning point clouds at the forest plot level. Methods in
Ecology and Evolution, 12(12):2473‚Äì2486, 2021. 3
[53] Di Wang, Stephane Momo Takoudjou, and Eric Casella. ¬¥
LeWoS: A universal leaf-wood classification method to facilitate the 3D modelling of large tropical trees using terrestrial
LiDAR. Methods in Ecology and Evolution, 11(3):376‚Äì389,
2019. 3
[54] Feiyu Wang and Mitch Bryson. Tree segmentation and
parameter measurement from point clouds using deep and
handcrafted features. Remote Sensing, 15(4):1086, 2023. 3
[55] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. SGPN: Similarity group proposal network for 3D
point cloud instance segmentation. In CVPR, pages 2569‚Äì
2578, 2018. 2, 5
[56] Xinlong Wang, Shu Liu, Xiaoyong Shen, Chunhua Shen, and
Jiaya Jia. Associatively segmenting instances and semantics
in point clouds. In CVPR, pages 4091‚Äì4100, 2019. 4, 5, 6, 1
[57] Maciej Wielgosz, Stefano Puliti, Phil Wilkes, and Rasmus
Astrup. Point2Tree(P2T)‚Äîframework for parameter tuning of semantic and instance segmentation used with mobile
laser scanning data in coniferous forest. Remote Sensing, 15
(15):3737, 2023. 3, 7
[58] Maciej Wielgosz, Stefano Puliti, Binbin Xiang, Konrad
Schindler, and Rasmus Astrup. SegmentAnyTree: A sensor
and platform agnostic deep learning model for tree segmentation using laser scanning data. Remote Sensing of Environment, 313:114367, 2024. 1, 3, 7, 2, 4, 5
[59] Phil Wilkes, Mathias Disney, John Armston, Harm
Bartholomeus, Lisa Bentley, Benjamin Brede, Andrew Burt,
Kim Calders, Cecilia Chavana-Bryant, Daniel Clewley,
Laura Duncanson, Brieanne Forbes, Sean Krisanski, Yadvinder Malhi, David Moffat, Niall Origo, Alexander Shenkin,
and Wanxin Yang. TLS2trees: A scalable tree segmentation
pipeline for TLS data. Methods in Ecology and Evolution,
14(12):3083‚Äì3099, 2023. 3, 7
[60] Lloyd Windrim and Mitch Bryson. Detection, segmentation,
and model fitting of individual tree stems from airborne laser
scanning of forests using deep learning. Remote Sensing, 12
(9):1469, 2020. 3
[61] Zhouxin Xi, Chris Hopkinson, and Laura Chasmer. Filtering stems and branches from terrestrial laser scanning point
clouds using deep 3-D fully convolutional networks. Remote
Sensing, 10(8):1215, 2018. 3
[62] Binbin Xiang, Torben Peters, Theodora Kontogianni, Frawa
Vetterli, Stefano Puliti, Rasmus Astrup, and Konrad
Schindler. Towards accurate instance segmentation in largescale LiDAR point clouds. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, X1/W1-2023:605‚Äì612, 2023. 4, 5, 1
[63] Binbin Xiang, Maciej Wielgosz, Theodora Kontogianni, Torben Peters, Stefano Puliti, Rasmus Astrup, and Konrad
Schindler. Automated forest inventory: Analysis of highdensity airborne LiDAR point clouds with 3D deep learning.
Remote Sensing of Environment, 305:114078, 2024. 1, 3, 4,
6, 2, 5
[64] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen
Wang, Andrew Markham, and Niki Trigoni. Learning object bounding boxes for 3D instance segmentation on point
clouds. In NIPS, pages 6740‚Äì6749, 2019. 2
[65] Lei Yao, Yi Wang, Moyun Liu, and Lap-Pui Chau. SGIFormer: Semantic-guided and geometric-enhanced interleaving transformer for 3D instance segmentation. arXiv
preprint arXiv:2407.11564, 2024. 3
[66] L. Yi, Wang Zhao, He Wang, Minhyuk Sung, and L. Guibas.
GSPN: Generative shape proposal network for 3D instance
segmentation in point cloud. In CVPR, pages 3942‚Äì3951,
2019. 2
[67] Feihu Zhang, Chenye Guan, Jin Fang, Song Bai, Ruigang
Yang, Philip H.S. Torr, and Victor Prisacariu. Instance segmentation of LiDAR point clouds. In IEEE International
Conference on Robotics and Automation (ICRA), pages
9448‚Äì9455, 2020. 2
[68] Yizhuo Zhang, Hantao Liu, Xingyu Liu, and Huiling Yu. Towards intricate stand structure: A novel individual tree segmentation method for ALS point cloud based on extreme offset deep learning. Applied Science, 13(11):6853, 2023. 3
[69] Min Zhong, Xinghao Chen, Xiaokang Chen, Gang Zeng, and
Yunhe Wang. MaskGroup: Hierarchical point grouping and
masking for 3D instance segmentation. In ICME, pages 1‚Äì6,
2022. 2
[70] Xiaochen Zhou, Bosheng Li, Bedrich Benes, Ayman Habib,
Songlin Fei, Jinyuan Shao, and Soren Pirk. TreeStructor: ¬®
Forest reconstruction with neural ranking. IEEE Transactions on Geoscience and Remote Sensing, 63:1‚Äì19, 2025. 3
[71] Zhuofan Zong, Guanglu Song, and Yu Liu. DETRs with
collaborative hybrid assignments training. In ICCV, pages
6725‚Äì6735, 2023. 5

